{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make it sure it's the right size\"\"\"\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "\n",
    "    if not os.path.exists('file_path'):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print('File already exists')\n",
    "\n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    if not os.path.exists(extract_path):\n",
    "\n",
    "        with zipfile.ZipFile(\n",
    "            os.path.join(data_dir,'bbc-fulltext.zip'),\n",
    "            'r'\n",
    "        ) as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "    \n",
    "    else:\n",
    "        print('bbc-fulltext,zip has already been extracted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n",
      "bbc-fulltext,zip has already been extracted\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_dir):\n",
    "    news_stories = []\n",
    "    print('Reading files')\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for fi, f in enumerate(files):\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "            print('.'*fi, f, end='\\r')\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
    "                story = []\n",
    "                for row in f:\n",
    "                    story.append(row.strip())\n",
    "                story = ''.join(story)\n",
    "                news_stories.append(story)\n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................ 284.txt........ 284.txt...................................................................................... 284.txt442.txt 187.txt 349.txt 194.txt\n",
      "Detected 2225 stories\n",
      "843863 words in the total news set\n",
      "Example words (starts): Musicians to tackle US red tapeMusicians' groups a\n",
      "Example words (end): Hacker threat to Apple's iTunesUsers of Apple's music jukebox iTunes need to update the software to avoid a potential security threat.Hackers can build malicious playlist files which could crash the program and let them seize control of the computer by inserting Trojan code. A new version of iTunes is now available from the Apple website which solves the problem. Security firm iDefence, which notified users of the problem, recommended that users upgrade to iTunes version 4.7.1. The problem affects all users of iTunes - Windows and Mac OS - running versions 4.7 and earlier. Users can automatically upgrade iTunes by opening the \"look for updates\" window in the program. The security firm says users should avoid clicking on or accessing playlist files - which have the file extension of .pls or .m3u - which have come from unknown sources. Itunes is the world's most popular online music store with more than 200 m\n"
     ]
    }
   ],
   "source": [
    "news_stories = read_data(os.path.join('data','bbc'))\n",
    "\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words in the total news set\")\n",
    "print('Example words (starts):', news_stories[0][:50])\n",
    "print('Example words (end):', news_stories[-1][:-50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 09:11:19.368781: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words = None,\n",
    "    filters = '!\"#$%&()*+,-/:;<=>?@[\\\\]^_{|}~\\t\\n',\n",
    "    lower = True,\n",
    "    split = ' '\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(news_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_index is a dictionary that maps each words to a unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 47187\n",
      "\n",
      "Words at the top\n",
      "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
      "\n",
      "Words at the bottom\n",
      "\t {'sp.': 47177, 'gameboy.': 47178, 'itunesusers': 47179, 'threat.hackers': 47180, 'inserting': 47181, 'solves': 47182, 'idefence': 47183, '4.7.1.': 47184, '.pls': 47185, '.m3u': 47186}\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(tokenizer.word_index.items()) + 1\n",
    "print(f\"Vocabulary size: {n_vocab}\")\n",
    "\n",
    "print(\"\\nWords at the top\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
    "print(\"\\nWords at the bottom\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index_word is a dictionary that maps each unique word ID to the corresponding word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 47187\n",
      "\n",
      "Words at the top\n",
      "\t {1: 'the', 2: 'to', 3: 'of', 4: 'and', 5: 'a', 6: 'in', 7: 'for', 8: 'is', 9: 'that', 10: 'on'}\n",
      "\n",
      "Words at the bottom\n",
      "\t {47177: 'sp.', 47178: 'gameboy.', 47179: 'itunesusers', 47180: 'threat.hackers', 47181: 'inserting', 47182: 'solves', 47183: 'idefence', 47184: '4.7.1.', 47185: '.pls', 47186: '.m3u'}\n"
     ]
    }
   ],
   "source": [
    "n_vocab_index_word = len(tokenizer.index_word.items()) + 1\n",
    "print(f\"Vocabulary size: {n_vocab_index_word}\")\n",
    "\n",
    "print(\"\\nWords at the top\")\n",
    "print('\\t', dict(list(tokenizer.index_word.items())[:10]))\n",
    "print(\"\\nWords at the bottom\")\n",
    "print('\\t', dict(list(tokenizer.index_word.items())[-10:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_2 = Tokenizer(\n",
    "    num_words = 15000,\n",
    "    filters = '!\"#$%&()*+,-/:;<=>?@[\\\\]^_{|}~\\t\\n',\n",
    "    lower = True,\n",
    "    split = ' ',\n",
    "    oov_token ='', #OOV = out of vocabulary: in this case the last 15000 words are out of vocabulary because they are more rare then the first 15000th\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_2.fit_on_texts(news_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Musicians to tackle US red tapeMusicians' groups are to tackle US visa regulations which are blamed \n"
     ]
    }
   ],
   "source": [
    "print(f'Original: {news_stories[0][:100]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence IDs: [2827, 3, 1310, 49, 1236, 1, 881, 24, 3, 1310, 49, 2756, 3851, 35, 24, 2142]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sequence IDs: {tokenizer_2.texts_to_sequences([news_stories[0][:100]])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample phrase: musicians to tackle us red\n",
      "Sample word IDs: [2826, 2, 1309, 48, 1235]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_word_ids = news_sequences[0][:5]\n",
    "sample_phrase = ' '.join([tokenizer.index_word[wid] for wid in sample_word_ids])\n",
    "print(f\"Sample phrase: {sample_phrase}\")\n",
    "print(f\"Sample word IDs: {sample_word_ids}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1 # How many words to consider left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    sequence=sample_word_ids,\n",
    "    vocabulary_size=n_vocab,\n",
    "    window_size=window_size,\n",
    "    negative_samples=1.0,\n",
    "    shuffle = False,\n",
    "    categorical = False,\n",
    "    sampling_table = None,\n",
    "    seed = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample skip-grams\n",
      "\tInput: [2826, 2](['ces', 'the']) /  Label: 1\n",
      "\tInput: [2, 2826](['the', 'ces']) /  Label: 1\n",
      "\tInput: [2, 1309](['the', 'improve']) /  Label: 1\n",
      "\tInput: [1309, 2](['improve', 'the']) /  Label: 1\n",
      "\tInput: [1309, 48](['improve', 'about']) /  Label: 1\n",
      "\tInput: [48, 1309](['about', 'improve']) /  Label: 1\n",
      "\tInput: [48, 1235](['about', 'premiership']) /  Label: 1\n",
      "\tInput: [1235, 48](['premiership', 'about']) /  Label: 1\n",
      "\tInput: [1235, 29223](['premiership', 'thankful.angels']) /  Label: 0\n",
      "\tInput: [1309, 21063](['improve', 'palepoi']) /  Label: 0\n",
      "\tInput: [1309, 30057](['improve', 'weidensteiner.']) /  Label: 0\n",
      "\tInput: [2826, 19699](['ces', 'schweppes.']) /  Label: 0\n",
      "\tInput: [48, 28784](['about', 'incarnation.']) /  Label: 0\n",
      "\tInput: [2, 26070](['the', 'rigueur']) /  Label: 0\n",
      "\tInput: [48, 22070](['about', 'zambian']) /  Label: 0\n",
      "\tInput: [2, 36920](['the', \"scots'\"]) /  Label: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample skip-grams\")\n",
    "\n",
    "for inp, lbl in zip(inputs,labels):\n",
    "    print(f\"\\tInput: {inp}({[tokenizer_2.index_word[wi] for wi in inp]}) /  Label: {lbl}\")\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating positive candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    sequence=sample_word_ids,\n",
    "    vocabulary_size=n_vocab,\n",
    "    window_size=window_size,\n",
    "    negative_samples=0,\n",
    "    shuffle = False,\n",
    ")\n",
    "\n",
    "inputs, labels = np.array(inputs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample skip grams\n",
      "Input: [2826    2] (['ces', 'the'])  / Label:1\n",
      "Input: [   2 2826] (['the', 'ces'])  / Label:1\n",
      "Input: [   2 1309] (['the', 'improve'])  / Label:1\n",
      "Input: [1309    2] (['improve', 'the'])  / Label:1\n",
      "Input: [1309   48] (['improve', 'about'])  / Label:1\n",
      "Input: [  48 1309] (['about', 'improve'])  / Label:1\n",
      "Input: [  48 1235] (['about', 'premiership'])  / Label:1\n",
      "Input: [1235   48] (['premiership', 'about'])  / Label:1\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample skip grams\")\n",
    "\n",
    "\n",
    "for inp, lbl in zip(inputs,labels):\n",
    "    print(f\"Input: {inp} ({[tokenizer_2.index_word[wi] for wi in inp]})  / Label:{lbl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating negative candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 09:11:26.965016: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "negative_sampling_candidates, true_excepted_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes = inputs[:1,1:], #[b, 1] sized tensor\n",
    "    num_true = 1, #number of true words per example\n",
    "    num_sampled = 10,\n",
    "    unique = True,\n",
    "    range_max = n_vocab,\n",
    "    name = 'negative_sampling'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sample: [[2]] \n",
      "Negative sample:[3759 1197 1015    0   80   11   18 1145   31  652]\n",
      "true_expected_count: [[0.26731545]]\n",
      "sampled_expected_count: [2.4709597e-04 7.7530619e-04 9.1412151e-04 6.4407539e-01 1.1401424e-02\n",
      " 7.4376032e-02 4.7661956e-02 8.1047055e-04 2.8593160e-02 1.4218892e-03]\n"
     ]
    }
   ],
   "source": [
    "print(f'Positive sample: {inputs[:1,1:]} ')\n",
    "print(f\"Negative sample:{negative_sampling_candidates}\")\n",
    "print(f\"true_expected_count: {true_excepted_count}\")\n",
    "print(f\"sampled_expected_count: {sampled_expected_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator function that generates batches of data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(\n",
    "        n_vocab, sampling_factor = 1e-05\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram_data_generator(sequences, window_size, batch_size, negative_samples, vocab_size, seed=None):\n",
    "    #data shuffling\n",
    "    rand_sequence_ids = np.arange(len(sequences))\n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "    #generating positive skip grams\n",
    "    for si in rand_sequence_ids:\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequences[si],\n",
    "            vocabulary_size= vocab_size,\n",
    "            window_size = window_size,\n",
    "            negative_samples= 0.0,\n",
    "            shuffle= False,\n",
    "            sampling_table= sampling_table, #using subsampling technique\n",
    "            seed = seed\n",
    "        )\n",
    "\n",
    "   \n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "        context_class = tf.expand_dims(tf.constant([context_word],dtype = 'int64'), 1)\n",
    "\n",
    "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "            true_classes = context_class,\n",
    "            num_true = 1,\n",
    "            num_sampled = negative_samples,\n",
    "            unique = True,\n",
    "            range_max = vocab_size,\n",
    "            name = 'negative_sampling'\n",
    "        )\n",
    "\n",
    "    # Build context and label vectors (for one taget word)\n",
    "    context = tf.concat(\n",
    "        [tf.constant([context_word], dtype = 'int64'), negative_sampling_candidates],\n",
    "        axis=0)\n",
    "    label = tf.constant([1] + [0]*negative_samples, dtype = 'int64')\n",
    "\n",
    "    #Append each element from the training example to global lists\n",
    "    targets.extend([target_word]*(negative_samples + 1))\n",
    "    contexts.append(context)\n",
    "    labels.append(label)\n",
    "\n",
    "    contexts, targets, labels = np.concatenate(contexts), np.array(targets), np.concatenate(labels)\n",
    "    #If seed is not provided generate a random number\n",
    "    if not seed:\n",
    "        seed = random.randint(0, 10e6)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(contexts)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(targets)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(labels)\n",
    "\n",
    "    #geraration batches of data\n",
    "    for eg_id_start in range(0, contexts.shape[0], batch_size):\n",
    "        yield(\n",
    "            targets[eg_id_start: min(eg_id_start + batch_size, inputs.shape[0])],\n",
    "            contexts[eg_id_start: min(eg_id_start + batch_size, inputs.shape[0])]\n",
    "            ),labels[eg_id_start: min(eg_id_start + batch_size, inputs.shape[0])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemeting the skip-gram architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =4096 #data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector\n",
    "\n",
    "window_size = 1 # Using a window size of 1 on either side of target word\n",
    "negative_samples = 4 # Number of negative samples generated per example\n",
    "\n",
    "epochs = 5 # Number of epochs to train for\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on\n",
    "valid_window = 250\n",
    "\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model using  Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clearing any current running session, to make sure there aren't any other models occupying the hardware\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs - skipgrams() function outputs target, context in that order \n",
    "input_1 = tf.keras.layers.Input(shape=(), name='target') # shape is defined has undefined dimension\n",
    "input_2 = tf.keras.layers.Input(shape=(), name = 'context') # shape is defined has undefined dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two embeddings layers are used one for the context and one for the target\n",
    "target_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim = n_vocab,\n",
    "    output_dim = embedding_size,\n",
    "    name = 'target_embedding'\n",
    ")\n",
    "context_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim = n_vocab,\n",
    "    output_dim = embedding_size,\n",
    "    name = 'context_embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup outputs of the embedding layers\n",
    "target_out = target_embedding_layer(input_1)\n",
    "context_out = context_embedding_layer(input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the dot product between the two\n",
    "out = tf.keras.layers.Dot(axes = -1)([context_out, target_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "skip_gram_model = tf.keras.models.Model(inputs = [input_1,input_2], outputs = out, name='skip_gram_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "skip_gram_model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"skip_gram_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " context (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " target (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 128)          6039936     ['context[0][0]']                \n",
      "                                                                                                  \n",
      " target_embedding (Embedding)   (None, 128)          6039936     ['target[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['context_embedding[0][0]',      \n",
      "                                                                  'target_embedding[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,079,872\n",
      "Trainable params: 12,079,872\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "skip_gram_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model and evaluating the model\n",
    "\n",
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "\n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        \"\"\"Validation logic\"\"\"\n",
    "\n",
    "        #Using context embeddings to get the most similar \n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis = 1, keepdims=True))\n",
    "\n",
    "        # Get the embeddings corresping to valid_term_ids \n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids,:]\n",
    "\n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "\n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "\n",
    "        #Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "\n",
    "            similar_word_str = ','.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j > 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}): {similar_word_str}\")\n",
    "\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 started\n",
      "      1/Unknown - 1s 755ms/step - loss: 0.6931 - accuracy: 0.8000labour): tass,stages.,adsmusic,filmhollywood,charge.in\n",
      "based): corruption.,144,dreamed,serene,provinces\n",
      "has): 193,1938.,glory.,two,vima\n",
      "you): nevo,fedorova,kitchen.,idols',offputting\n",
      "also): dripped,â£24.8m,playability,parliament.however,robbie's\n",
      "between): vouch,keegan.,together.any,phoned,hornsey\n",
      "go): 1921,davies,1985's,trusted.,khoo\n",
      "film): buy.,activities,existence.,â£28.7m,msps.the\n",
      "who): scots,thayer,5.11,nottinghamshire,cruel\n",
      "another): predominately,lapsed,426m,underestimating,decorating\n",
      "make): fonda,extortionists,sailed,warwick's,hardcourts\n",
      "since): turkcell's,low.the,printing,liverpoolbournemouth,entrenches\n",
      "me): dealings,migrated,2004.however,thanou.,spend.\n",
      "so): remotes,fold.the,found,monteiro,'victory'\n",
      "added): â£280.2m,higher.,woman.,juan,nestle's\n",
      "mr): francewales,volleyed,surge,limbo,yoxall\n",
      "figure): achieve.,kse,counts,lea,fis.\n",
      "roddick): 271st,convener,wounded,eurozone,scrum'.\n",
      "income): safeguarded.,paypal,ruin,stelios,1947.\n",
      "wins): work,bruni,latham,speedy,'minor'\n",
      "attempt): interned,16.2bn,evermore,swerving,enduring\n",
      "that.): washington,52.bevan,uk.subscribers,firm.speaking,secretary\n",
      "wide): time.my,violations.,davos,spammers,dementieva\n",
      "nominated): â£4.25m,evolve,bundled,atlantic,burnout\n",
      "bankruptcy): maduaka.,prints.,paralysed,goldsmith,objections.he\n",
      "highest): experience,furore,cable.,enarsa.,yauch\n",
      "james): tackle,crackdowns,recession.,ebu,channel\n",
      "separate): saxophone,equalisers,joko,elections,resignation.but\n",
      "gadgets): nightfire,nieminen,717,'anti,diallers\n",
      "lives): learning.,gui,elan's,loading,side.\n",
      "millions): men's,rural,setback.,flight's,patch\n",
      "confident): 'dump',pantech,flynn,uncharacteristic,delisted\n",
      "\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6931 - accuracy: 0.8000\n",
      "Epoch: 2/5 started\n",
      "      1/Unknown - 0s 35ms/step - loss: 0.6921 - accuracy: 0.8000labour): tass,stages.,adsmusic,filmhollywood,charge.in\n",
      "based): corruption.,144,dreamed,serene,provinces\n",
      "has): 193,1938.,glory.,two,vima\n",
      "you): nevo,fedorova,kitchen.,idols',offputting\n",
      "also): dripped,â£24.8m,playability,parliament.however,robbie's\n",
      "between): vouch,keegan.,together.any,phoned,hornsey\n",
      "go): 1921,davies,1985's,trusted.,khoo\n",
      "film): buy.,activities,existence.,â£28.7m,msps.the\n",
      "who): scots,thayer,5.11,nottinghamshire,cruel\n",
      "another): predominately,lapsed,426m,underestimating,decorating\n",
      "make): fonda,extortionists,sailed,warwick's,hardcourts\n",
      "since): turkcell's,low.the,printing,liverpoolbournemouth,entrenches\n",
      "me): dealings,migrated,2004.however,thanou.,spend.\n",
      "so): remotes,fold.the,found,monteiro,'victory'\n",
      "added): â£280.2m,higher.,woman.,juan,nestle's\n",
      "mr): francewales,volleyed,surge,limbo,yoxall\n",
      "figure): achieve.,kse,counts,lea,fis.\n",
      "roddick): 271st,convener,wounded,eurozone,scrum'.\n",
      "income): safeguarded.,paypal,ruin,stelios,1947.\n",
      "wins): work,bruni,latham,speedy,'minor'\n",
      "attempt): interned,16.2bn,evermore,swerving,enduring\n",
      "that.): washington,52.bevan,uk.subscribers,firm.speaking,secretary\n",
      "wide): time.my,violations.,davos,spammers,dementieva\n",
      "nominated): â£4.25m,evolve,bundled,atlantic,burnout\n",
      "bankruptcy): maduaka.,prints.,paralysed,goldsmith,objections.he\n",
      "highest): experience,furore,cable.,enarsa.,yauch\n",
      "james): tackle,crackdowns,recession.,ebu,channel\n",
      "separate): saxophone,equalisers,joko,elections,resignation.but\n",
      "gadgets): nightfire,nieminen,717,'anti,diallers\n",
      "lives): learning.,gui,elan's,loading,side.\n",
      "millions): men's,rural,setback.,flight's,patch\n",
      "confident): 'dump',pantech,flynn,uncharacteristic,delisted\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.6921 - accuracy: 0.8000\n",
      "Epoch: 3/5 started\n",
      "      1/Unknown - 0s 29ms/step - loss: 0.6925 - accuracy: 0.8000labour): tass,stages.,adsmusic,filmhollywood,charge.in\n",
      "based): corruption.,144,dreamed,serene,provinces\n",
      "has): 193,1938.,glory.,two,vima\n",
      "you): nevo,fedorova,kitchen.,idols',offputting\n",
      "also): dripped,â£24.8m,playability,parliament.however,robbie's\n",
      "between): vouch,keegan.,together.any,phoned,hornsey\n",
      "go): 1921,davies,1985's,trusted.,khoo\n",
      "film): buy.,activities,existence.,â£28.7m,msps.the\n",
      "who): scots,thayer,5.11,nottinghamshire,cruel\n",
      "another): predominately,lapsed,426m,underestimating,decorating\n",
      "make): fonda,extortionists,sailed,warwick's,hardcourts\n",
      "since): turkcell's,low.the,printing,liverpoolbournemouth,entrenches\n",
      "me): dealings,migrated,2004.however,thanou.,spend.\n",
      "so): remotes,fold.the,found,monteiro,'victory'\n",
      "added): â£280.2m,higher.,woman.,juan,nestle's\n",
      "mr): francewales,volleyed,surge,limbo,yoxall\n",
      "figure): achieve.,kse,counts,lea,fis.\n",
      "roddick): 271st,convener,wounded,eurozone,scrum'.\n",
      "income): safeguarded.,paypal,ruin,stelios,1947.\n",
      "wins): work,bruni,latham,speedy,'minor'\n",
      "attempt): interned,16.2bn,evermore,swerving,enduring\n",
      "that.): washington,52.bevan,uk.subscribers,firm.speaking,secretary\n",
      "wide): time.my,violations.,davos,spammers,dementieva\n",
      "nominated): â£4.25m,evolve,bundled,atlantic,burnout\n",
      "bankruptcy): maduaka.,prints.,paralysed,goldsmith,objections.he\n",
      "highest): experience,furore,cable.,enarsa.,yauch\n",
      "james): tackle,crackdowns,recession.,ebu,channel\n",
      "separate): saxophone,equalisers,joko,elections,resignation.but\n",
      "gadgets): nightfire,nieminen,717,'anti,diallers\n",
      "lives): learning.,gui,elan's,loading,side.\n",
      "millions): men's,rural,setback.,flight's,patch\n",
      "confident): 'dump',pantech,flynn,uncharacteristic,delisted\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.6925 - accuracy: 0.8000\n",
      "Epoch: 4/5 started\n",
      "      1/Unknown - 0s 32ms/step - loss: 0.6932 - accuracy: 0.8000labour): tass,stages.,adsmusic,filmhollywood,charge.in\n",
      "based): corruption.,144,dreamed,serene,provinces\n",
      "has): 193,1938.,glory.,two,vima\n",
      "you): nevo,fedorova,kitchen.,idols',offputting\n",
      "also): dripped,â£24.8m,playability,parliament.however,robbie's\n",
      "between): vouch,keegan.,together.any,phoned,hornsey\n",
      "go): 1921,davies,1985's,trusted.,khoo\n",
      "film): buy.,activities,existence.,â£28.7m,msps.the\n",
      "who): scots,thayer,5.11,nottinghamshire,cruel\n",
      "another): predominately,lapsed,426m,underestimating,decorating\n",
      "make): fonda,extortionists,sailed,warwick's,hardcourts\n",
      "since): turkcell's,low.the,printing,liverpoolbournemouth,entrenches\n",
      "me): dealings,migrated,2004.however,thanou.,spend.\n",
      "so): remotes,fold.the,found,monteiro,'victory'\n",
      "added): â£280.2m,higher.,woman.,juan,nestle's\n",
      "mr): francewales,volleyed,surge,limbo,yoxall\n",
      "figure): achieve.,kse,counts,lea,fis.\n",
      "roddick): 271st,convener,wounded,eurozone,scrum'.\n",
      "income): safeguarded.,paypal,ruin,stelios,1947.\n",
      "wins): work,bruni,latham,speedy,'minor'\n",
      "attempt): interned,16.2bn,evermore,swerving,enduring\n",
      "that.): washington,52.bevan,uk.subscribers,firm.speaking,secretary\n",
      "wide): time.my,violations.,davos,spammers,dementieva\n",
      "nominated): â£4.25m,evolve,bundled,atlantic,burnout\n",
      "bankruptcy): maduaka.,prints.,paralysed,goldsmith,objections.he\n",
      "highest): experience,furore,cable.,enarsa.,yauch\n",
      "james): tackle,crackdowns,recession.,ebu,channel\n",
      "separate): saxophone,equalisers,joko,elections,resignation.but\n",
      "gadgets): nightfire,nieminen,717,'anti,diallers\n",
      "lives): learning.,gui,elan's,loading,side.\n",
      "millions): men's,rural,setback.,flight's,patch\n",
      "confident): 'dump',pantech,flynn,uncharacteristic,delisted\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.6932 - accuracy: 0.8000\n",
      "Epoch: 5/5 started\n",
      "      1/Unknown - 0s 28ms/step - loss: 0.6940 - accuracy: 0.8000labour): tass,stages.,adsmusic,filmhollywood,charge.in\n",
      "based): corruption.,144,dreamed,serene,provinces\n",
      "has): 193,1938.,glory.,two,vima\n",
      "you): nevo,fedorova,kitchen.,idols',offputting\n",
      "also): dripped,â£24.8m,playability,parliament.however,robbie's\n",
      "between): vouch,keegan.,together.any,phoned,hornsey\n",
      "go): 1921,davies,1985's,trusted.,khoo\n",
      "film): buy.,activities,existence.,â£28.7m,msps.the\n",
      "who): scots,thayer,5.11,nottinghamshire,cruel\n",
      "another): predominately,lapsed,426m,underestimating,decorating\n",
      "make): fonda,extortionists,sailed,warwick's,hardcourts\n",
      "since): turkcell's,low.the,printing,liverpoolbournemouth,entrenches\n",
      "me): dealings,migrated,2004.however,thanou.,spend.\n",
      "so): remotes,fold.the,found,monteiro,'victory'\n",
      "added): â£280.2m,higher.,woman.,juan,nestle's\n",
      "mr): francewales,volleyed,surge,limbo,yoxall\n",
      "figure): achieve.,kse,counts,lea,fis.\n",
      "roddick): 271st,convener,wounded,eurozone,scrum'.\n",
      "income): safeguarded.,paypal,ruin,stelios,1947.\n",
      "wins): work,bruni,latham,speedy,'minor'\n",
      "attempt): interned,16.2bn,evermore,swerving,enduring\n",
      "that.): washington,52.bevan,uk.subscribers,firm.speaking,secretary\n",
      "wide): time.my,violations.,davos,spammers,dementieva\n",
      "nominated): â£4.25m,evolve,bundled,atlantic,burnout\n",
      "bankruptcy): maduaka.,prints.,paralysed,goldsmith,objections.he\n",
      "highest): experience,furore,cable.,enarsa.,yauch\n",
      "james): tackle,crackdowns,recession.,ebu,channel\n",
      "separate): saxophone,equalisers,joko,elections,resignation.but\n",
      "gadgets): nightfire,nieminen,717,'anti,diallers\n",
      "lives): learning.,gui,elan's,loading,side.\n",
      "millions): men's,rural,setback.,flight's,patch\n",
      "confident): 'dump',pantech,flynn,uncharacteristic,delisted\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.6940 - accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "skipgram_validation_callback = ValidationCallback(valid_term_ids, skip_gram_model, tokenizer)\n",
    "\n",
    "for ei in range(epochs):\n",
    "\n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "\n",
    "    news_skip_gram_gen = skip_gram_data_generator(\n",
    "        news_sequences, window_size, batch_size, negative_samples, n_vocab\n",
    "    )\n",
    "\n",
    "    skip_gram_model.fit(\n",
    "        news_skip_gram_gen, epochs = 1,\n",
    "        callbacks = skipgram_validation_callback,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tensorflow_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c46df576936c8255b1c2554cedc1eb44982b877a37736dc6e33d18af7b82b03d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
